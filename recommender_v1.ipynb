{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from sb3_contrib import TRPO\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from enum import IntEnum\n",
    "import numpy as np\n",
    "import pickle\n",
    "from akt import AKT\n",
    "import os\n",
    "from load_data import DATA, PID_DATA\n",
    "import csv\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PracticeProblemEnv(gym.Env):\n",
    "    def __init__(self, params, max_step=10, rew_func=\"mock\", units=None, device=\"cuda\"):\n",
    "        super(PracticeProblemEnv, self).__init__()\n",
    "        # self.curr_step = None\n",
    "        self.max_step = max_step\n",
    "        self.rew_func = rew_func\n",
    "        # self.curr_q = -1\n",
    "        # self.curr_pred = -1\n",
    "        # self.curr_pid = -1\n",
    "\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "        self.kt_model = self._load_model()\n",
    "        self.p_q_dict, self.q_p_dict = self._load_pq_qp_dict(units)\n",
    "\n",
    "        self.actions = [*self.p_q_dict.keys()]\n",
    "        self.action_space = spaces.Discrete(len(self.actions)) #[0,n_question-1]\n",
    "        self.observation_space = spaces.Box(np.array([1,0,1]), np.array([self.params.n_question, 1, self.params.n_pid])) #[1,n_question]/[0,1]/[1,n_pid]\n",
    "   \n",
    "\n",
    "\n",
    "        self.reset()\n",
    "    \n",
    "    def _load_model(self, pretrained_path=\"_b24_nb1_gn-1_lr1e-05_s224_sl200_do0.05_dm256_ts1_kq1_l21e-05_178\"):\n",
    "        model = AKT(n_question=self.params.n_question, n_pid=self.params.n_pid, n_blocks=self.params.n_block, d_model=self.params.d_model,\n",
    "                    dropout=self.params.dropout, kq_same=self.params.kq_same, model_type='akt', l2=self.params.l2).to(self.device)\n",
    "        checkpoint = torch.load(os.path.join( 'model', self.params.model, self.params.save, pretrained_path))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.history = {'q':[],'target':[],'pid':[]} # Initialize past interactions\n",
    "        self.curr_step = 0\n",
    "        self.curr_q = -1\n",
    "        self.curr_pred = -1\n",
    "        self.curr_pid = -1\n",
    "\n",
    "        # return self.step(np.random.choice(range(self.n_problems)))[0], {}\n",
    "        return self.step(self.action_space.sample())[0], {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([self.curr_q, self.curr_pred, self.curr_pid], dtype=int)\n",
    "        \n",
    "    def _rew(self, n_problems_per_type=1):\n",
    "        sampled_concpets = []\n",
    "        sampled_problems = []\n",
    "        for question_type, question_ids in self.q_p_dict.items():\n",
    "            num = min(n_problems_per_type, len(question_ids))\n",
    "            sampled_problems += random.sample([*question_ids], num)\n",
    "            sampled_concpets += ([question_type ] * num)\n",
    "\n",
    "        mean_performance = self.predict(sampled_concpets,sampled_problems)\n",
    "        return mean_performance\n",
    "    \n",
    "    def switch_rew(self, new_rew_func):\n",
    "        self.rew_func = new_rew_func\n",
    "    \n",
    "    def step(self, action):#action is an np int e.g. nparray(3) of the index of the action specified in self.actions\n",
    "        self.curr_step += 1\n",
    "        # self.curr_pid = self.actions[action.item]\n",
    "        self.curr_pid = self.actions[action]\n",
    "        self.curr_q = self.p_q_dict[self.curr_pid]\n",
    "\n",
    "        correct_prob = self.predict([self.curr_q],[self.curr_pid])\n",
    "        # correct_prob = self.predict()\n",
    "        self.curr_pred = np.random.rand() < correct_prob\n",
    "\n",
    "        if self.rew_func == \"mock\":\n",
    "            reward = self._rew()\n",
    "        elif self.rew_func == \"correct\":\n",
    "            reward = 1 if self.curr_pred else 0\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        # Update history with the action and the correctness\n",
    "        self.history['q'] += [self.curr_q]\n",
    "        self.history['target'] += [self.curr_pred]\n",
    "        self.history['pid'] += [self.curr_pid]\n",
    "        \n",
    "        # Recompute the state using the kt_model for each question\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        done = self.curr_step >= self.max_step\n",
    "        good = (self.rew_func == \"mock\") and (reward > 0.9)\n",
    "\n",
    "        return obs, reward, done, good, {}\n",
    "    \n",
    "    # def predict(self):\n",
    "    #     q = torch.tensor(self.history['q'][-(self.params.seqlen-1):]+[self.curr_q])\n",
    "    #     target = torch.tensor(self.history['target'][-(self.params.seqlen-1):]+[0])\n",
    "    #     pid = torch.tensor(self.history['pid'][-(self.params.seqlen-1):]+[self.curr_pid])\n",
    "    #     qa = q+target*self.params.n_question\n",
    "\n",
    "    #     padded_q = torch.zeros((1, self.params.seqlen))\n",
    "    #     padded_qa = torch.zeros((1, self.params.seqlen))\n",
    "    #     padded_target = torch.full((1,self.params.seqlen),-1)\n",
    "    #     padded_pid = torch.zeros((1, self.params.seqlen))\n",
    "\n",
    "    #     pred_index = q.shape[0]\n",
    "    #     padded_q[:, :len(q)] = q\n",
    "    #     padded_qa[:, :len(q)] = qa\n",
    "    #     padded_target[:, :len(target)] = target\n",
    "    #     padded_pid[:, :len(pid)] = pid\n",
    "\n",
    "    #     # target_1 = np.floor(target)\n",
    "    #     q = padded_q.long().to(self.device)\n",
    "    #     qa = padded_qa.long().to(self.device)\n",
    "    #     target = padded_target.long().to(self.device)\n",
    "    #     pid = padded_pid.long().to(self.device)\n",
    "        \n",
    "    #     with torch.no_grad():\n",
    "    #         loss, pred, ct = self.kt_model(q,qa,target,pid)\n",
    "\n",
    "    #     nopadding_index = np.flatnonzero(padded_target.reshape((-1,)) >= -0.9).tolist()\n",
    "    #     pred_nopadding = pred[nopadding_index]\n",
    "    #     correct_prob = pred_nopadding[-1].item()\n",
    "    #     return correct_prob\n",
    "    \n",
    "    def predict(self, curr_q, curr_pid):\n",
    "\n",
    "        assert type(curr_q) == type(curr_pid) == list\n",
    "        batch_size = len(curr_q)\n",
    "\n",
    "        q = torch.cat((torch.tensor(self.history['q'][-(self.params.seqlen-1):]).tile((batch_size,1)),(torch.tensor(curr_q).unsqueeze(-1))),1)\n",
    "        target = torch.tensor(self.history['target'][-(self.params.seqlen-1):]+[0]).tile((batch_size,1))\n",
    "        pid = torch.cat((torch.tensor(self.history['pid'][-(self.params.seqlen-1):]).tile((batch_size,1)),(torch.tensor(curr_pid).unsqueeze(-1))),1)\n",
    "        assert pid.shape == target.shape == pid.shape #(test_n_problem,3)\n",
    "        qa = q+target*self.params.n_question\n",
    "        \n",
    "        padded_q = torch.zeros((batch_size, self.params.seqlen))\n",
    "        padded_qa = torch.zeros((batch_size, self.params.seqlen))\n",
    "        padded_target = torch.full((batch_size,self.params.seqlen),-1)\n",
    "        padded_pid = torch.zeros((batch_size, self.params.seqlen))\n",
    "\n",
    "        pred_index = q.shape[1]\n",
    "        padded_q[:, :pred_index]= q\n",
    "        padded_qa[:, :pred_index]= qa\n",
    "        padded_target[:, :pred_index]= target\n",
    "        padded_pid[:, :pred_index]= pid\n",
    "\n",
    "        q = padded_q.long().to(device)\n",
    "        qa = padded_qa.long().to(device)\n",
    "        target = padded_target.long().to(device)\n",
    "        pid = padded_pid.long().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, pred, ct = self.kt_model(q,qa,target,pid)\n",
    "\n",
    "        nopadding_index = np.flatnonzero(padded_target.reshape((-1,)) >= -0.9).tolist()\n",
    "        pred_nopadding = pred[nopadding_index]\n",
    "\n",
    "        test_result = pred_nopadding[(pred_index-1)::pred_index]\n",
    "        assert test_result.shape == (batch_size,)\n",
    "        correct_prob = test_result.mean().item()\n",
    "\n",
    "        return correct_prob\n",
    "\n",
    "    def _load_pq_qp_dict(self, units=None):\n",
    "        def iterate_over_data(file_path):\n",
    "            with open(file_path, mode='r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                rows = list(reader)\n",
    "\n",
    "            for i in range(0, len(rows), 4):\n",
    "                # Extract the question ids and concept ids\n",
    "                question_ids = [int(q) for q in rows[i+1] if q]\n",
    "                concept_ids = [int(c) for c in rows[i+2] if c]\n",
    "                q_c_ids = [(int(q),int(c)) for (q,c) in zip(rows[i+1],rows[i+2]) if (q and c and ((units is None) or (int(c) in units)))]\n",
    "\n",
    "                # Build the dictionary mapping question ids to concept ids\n",
    "                for question_id, concept_id in q_c_ids:\n",
    "                    if question_id not in question_concept_dict:\n",
    "                        question_concept_dict[question_id] = concept_id\n",
    "                    if concept_id not in concept_question_dict:\n",
    "                        concept_question_dict[concept_id] = {question_id}\n",
    "                    else:\n",
    "                        concept_question_dict[concept_id].add(question_id)\n",
    "                        concept_question_dict[concept_id].add(question_id)\n",
    "\n",
    "        all_files = os.listdir(self.params.data_dir)\n",
    "\n",
    "        # Filter the list to include only CSV files\n",
    "        csv_files = [file for file in all_files if file.endswith('.csv')]\n",
    "\n",
    "        question_concept_dict = {}\n",
    "        concept_question_dict = {}\n",
    "        for f in csv_files:\n",
    "            old_dict = question_concept_dict.copy()\n",
    "            iterate_over_data(self.params.data_dir+'/'+ f)\n",
    "            if old_dict == question_concept_dict:\n",
    "                break\n",
    "        \n",
    "        return question_concept_dict, concept_question_dict\n",
    "    \n",
    "    def check_hist(self):\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "with open('result/akt_pid/assist2009_pid/args.pkl', 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "env = PracticeProblemEnv(params,max_step=10, rew_func='mock', units = [1], device=device)\n",
    "\n",
    "# Define and train the TRPO model\n",
    "model = TRPO('MlpPolicy', env, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.5      |\n",
      "|    ep_rew_mean     | 2.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 46       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.trpo.trpo.TRPO at 0x19abebafd60>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model_1000_100_mock1\")\n",
    "# model.load(\"model_1000_100_mock1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "# model.load(\"model_1000_100_mock1\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "env.switch_rew(\"mock\")\n",
    "obs_list = []\n",
    "total_rewards = []\n",
    "k=1000\n",
    "for _ in range(k):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, terminated, truncated, info = env.step(action)\n",
    "    total_rewards.append(rewards)\n",
    "    obs_list.append(obs)\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[109, 94, 72, 85, 52, 108, 39, 82, 48, 88, 7, 8, 13, 76, 92, 113, 106, 8, 119, 89, 110, 95, 115, 18, 41, 19, 18, 101, 64, 83, 23, 37, 123, 28, 59, 68, 63, 117, 86, 63, 82, 49, 90, 84, 54, 3, 80, 67, 50, 1, 19, 114, 125, 70, 125, 84, 107, 55, 89, 116, 58, 18, 48, 51, 110, 31, 13, 33, 73, 98, 57, 24, 21, 98, 41, 9, 6, 28, 5, 37, 122, 118, 132, 121, 69, 60, 33, 45, 62, 85, 43, 75, 78, 38, 19, 37, 97, 85, 17, 6, 85, 70, 3, 9, 114, 112, 15, 12, 104, 130, 57, 113, 8, 118, 127, 129, 57, 118, 14, 46, 51, 124, 79, 105, 41, 76, 108, 45, 94, 64, 106, 92, 92, 104, 25, 76, 15, 117, 105, 117, 77, 28, 15, 108, 36, 67, 16, 34, 64, 84, 22, 24, 17, 99, 118, 113, 8, 120, 55, 34, 127, 82, 65, 44, 12, 29, 5, 68, 87, 112, 8, 115, 78, 109, 2, 1, 37, 93, 94, 121, 2, 38, 101, 13, 86, 114, 95, 79, 131, 111, 93, 58, 17, 116, 98, 76, 1, 72, 60, 13, 4, 120, 75, 115, 94, 55, 5, 25, 46, 46, 129, 22, 6, 15, 118, 79, 10, 87, 109, 33, 16, 63, 56, 64, 30, 4, 36, 58, 4, 8, 8, 23, 22, 57, 37, 43, 112, 90, 50, 116, 117, 81, 91, 15, 82, 29, 13, 5, 7, 88, 23, 91, 90, 4, 22, 77, 79, 75, 49, 22, 1, 104, 74, 4, 129, 64, 76, 112, 106, 27, 8, 87, 75, 84, 11, 22, 122, 101, 37, 92, 75, 14, 107, 114, 24, 113, 117, 25, 122, 55, 23, 88, 80, 84, 94, 125, 34, 8, 132, 110, 82, 1, 34, 18, 27, 41, 59, 20, 68, 39, 84, 109, 56, 61, 92, 35, 19, 29, 51, 33, 33, 123, 91, 24, 67, 70, 42, 30, 36, 98, 128, 64, 92, 123, 1, 95, 108, 80, 112, 119, 66, 123, 30, 63, 92, 74, 12, 129, 110, 76, 54, 125, 113, 114, 77, 119, 92, 71, 81, 9, 117, 8, 16, 104, 42, 84, 34, 100, 92, 77, 76, 98, 115, 53, 29, 8, 36, 90, 88, 96, 96, 54, 20, 26, 132, 94, 22, 11, 116, 17, 30, 36, 72, 6, 4, 132, 17, 45, 131, 124, 88, 71, 124, 60, 82, 24, 86, 76, 119, 54, 95, 22, 100, 63, 20, 26, 58, 91, 3, 57, 105, 32, 64, 97, 118, 35, 88, 105, 22, 16, 111, 17, 118, 86, 47, 7, 31, 120, 108, 107, 39, 29, 121, 39, 128, 131, 58, 40, 26, 7, 46, 48, 11, 121, 66, 85, 52, 38, 113, 23, 55, 20, 102, 97, 7, 43, 76, 43, 99, 38, 38, 125, 21, 36, 84, 32, 18, 41, 72, 86, 67, 34, 130, 95, 117, 107, 4, 121, 21, 124, 93, 52, 22, 20, 98, 23, 104, 85, 101, 66, 73, 2, 98, 32, 132, 129, 116, 22, 127, 124, 27, 102, 115, 12, 132, 57, 53, 45, 20, 117, 107, 70, 123, 8, 98, 123, 84, 84, 19, 109, 96, 66, 130, 56, 14, 2, 52, 120, 1, 3, 72, 40, 16, 80, 76, 41, 70, 109, 128, 22, 102, 97, 21, 123, 17, 51, 54, 17, 75, 73, 79, 130, 31, 45, 99, 92, 114, 23, 32, 4, 120, 2, 129, 13, 99, 43, 103, 107, 100, 17, 105, 130, 108, 68, 123, 61, 127, 67, 55, 117, 93, 112, 45, 68, 78, 67, 93, 90, 122, 111, 62, 5, 70, 26, 27, 89, 59, 127, 37, 110, 29, 106, 116, 113, 70, 35, 8, 5, 72, 105, 130, 21, 13, 64, 88, 106, 122, 5, 40, 80, 37, 41, 49, 89, 88, 103, 15, 60, 19, 42, 78, 88, 84, 61, 44, 86, 67, 131, 8, 6, 106, 114, 76, 45, 4, 113, 127, 120, 47, 82, 10, 4, 98, 83, 113, 59, 40, 61, 124, 108, 51, 95, 42, 14, 117, 126, 94, 85, 19, 60, 127, 48, 126, 2, 105, 61, 27, 79, 125, 30, 104, 60, 80, 91, 31, 93, 94, 57, 93, 50, 104, 13, 6, 58, 72, 81, 46, 103, 122, 28, 92, 41, 55, 17, 49, 15, 60, 15, 70, 101, 38, 58, 125, 131, 76, 69, 15, 103, 100, 69, 50, 102, 102, 50, 7, 2, 6, 50, 111, 32, 24, 115, 101, 116, 123, 91, 113, 107, 57, 13, 106, 76, 53, 73, 121, 77, 103, 76, 15, 12, 111, 30, 13, 130, 121, 12, 90, 55, 30, 8, 59, 86, 42, 50, 82, 101, 38, 123, 99, 80, 61, 56, 120, 65, 91, 53, 20, 31, 42, 130, 42, 104, 10, 86, 95, 109, 92, 9, 24, 32, 126, 37, 62, 31, 132, 12, 23, 117, 32, 84, 33, 119, 75, 72, 130, 105, 27, 126, 126, 25, 69, 111, 49, 125, 104, 101, 5, 45, 118, 3, 3, 42, 17, 31, 112, 23, 94, 90, 128, 5, 48, 125, 111, 44, 85, 127, 68, 78, 112, 34, 84, 105, 3, 30, 29, 29, 128, 104, 99, 21, 8, 37, 31, 93, 22, 124, 52, 131, 5, 88, 117, 40, 76, 28, 54, 98, 78, 14, 110, 65, 5, 7, 93, 49, 81, 129, 90, 126, 11, 40, 126, 65, 47, 127, 125, 24, 20, 42, 99, 99, 102, 42, 83, 9, 60, 32, 106, 50, 67, 118, 90, 35, 15, 88, 4, 90, 129, 25, 112, 9, 114, 73, 123, 3, 81, 106, 61, 1, 94, 110, 93, 8, 48, 101, 81, 31, 20, 116, 82, 132, 105, 110, 8, 41, 132, 86, 7, 28, 75, 64, 9, 36, 8, 124, 3, 114, 117, 78, 77, 54, 88, 90, 71, 36, 23, 70, 11, 111, 41, 26, 121, 35, 44, 74, 116, 49, 5, 24, 132, 98, 125, 116, 89, 33, 75, 105, 62, 74, 74, 16, 71, 120, 82, 1, 57, 110, 102, 51, 74, 112]\n"
     ]
    }
   ],
   "source": [
    "print([i[0] for i in obs_list])\n",
    "print([i[2] for i in obs_list])\n",
    "# print(total_rewards/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2684689462184906,\n",
       " 0.19569899141788483,\n",
       " 0.18130241334438324,\n",
       " 0.21762630343437195,\n",
       " 0.2262425273656845,\n",
       " 0.1859651654958725,\n",
       " 0.21663598716259003,\n",
       " 0.23775117099285126,\n",
       " 0.43738433718681335,\n",
       " 0.899749755859375,\n",
       " 0.9113047122955322,\n",
       " 0.9187636375427246,\n",
       " 0.8796201944351196,\n",
       " 0.8884856700897217,\n",
       " 0.9255806803703308,\n",
       " 0.9033583998680115,\n",
       " 0.9003592729568481,\n",
       " 0.9063846468925476,\n",
       " 0.9144624471664429,\n",
       " 0.9157002568244934,\n",
       " 0.2179296463727951,\n",
       " 0.24461352825164795,\n",
       " 0.19291038811206818,\n",
       " 0.18481090664863586,\n",
       " 0.211447075009346,\n",
       " 0.19316084682941437,\n",
       " 0.2020760029554367,\n",
       " 0.5253486633300781,\n",
       " 0.4389297664165497,\n",
       " 0.19872157275676727,\n",
       " 0.21851469576358795,\n",
       " 0.6145160794258118,\n",
       " 0.8044691681861877,\n",
       " 0.808168888092041,\n",
       " 0.823475182056427,\n",
       " 0.8848199248313904,\n",
       " 0.8527151942253113,\n",
       " 0.8900446891784668,\n",
       " 0.880946159362793,\n",
       " 0.8938531279563904,\n",
       " 0.9023830890655518,\n",
       " 0.19684861600399017,\n",
       " 0.2348404973745346,\n",
       " 0.6312023997306824,\n",
       " 0.7603068351745605,\n",
       " 0.7881522178649902,\n",
       " 0.821006715297699,\n",
       " 0.8756198287010193,\n",
       " 0.9158852696418762,\n",
       " 0.166555717587471,\n",
       " 0.20547163486480713,\n",
       " 0.24627935886383057,\n",
       " 0.20405660569667816,\n",
       " 0.21985767781734467,\n",
       " 0.1707647442817688,\n",
       " 0.2313821017742157,\n",
       " 0.15706095099449158,\n",
       " 0.22395965456962585,\n",
       " 0.8814433217048645,\n",
       " 0.883273720741272,\n",
       " 0.8940236568450928,\n",
       " 0.9136335849761963,\n",
       " 0.19467829167842865,\n",
       " 0.2206842601299286,\n",
       " 0.21414481103420258,\n",
       " 0.2050887644290924,\n",
       " 0.21762551367282867,\n",
       " 0.1813098043203354,\n",
       " 0.19148056209087372,\n",
       " 0.17174570262432098,\n",
       " 0.22788184881210327,\n",
       " 0.8930452466011047,\n",
       " 0.8944445252418518,\n",
       " 0.878479540348053,\n",
       " 0.91350919008255,\n",
       " 0.9006474614143372,\n",
       " 0.9175097346305847,\n",
       " 0.9159045815467834,\n",
       " 0.17098009586334229,\n",
       " 0.21143002808094025,\n",
       " 0.18546853959560394,\n",
       " 0.1880125105381012,\n",
       " 0.251507967710495,\n",
       " 0.6486744284629822,\n",
       " 0.44290199875831604,\n",
       " 0.36776429414749146,\n",
       " 0.5037883520126343,\n",
       " 0.894702136516571,\n",
       " 0.9253904819488525,\n",
       " 0.9041430950164795,\n",
       " 0.9132649302482605,\n",
       " 0.20065031945705414,\n",
       " 0.27695614099502563,\n",
       " 0.20323342084884644,\n",
       " 0.17053855955600739,\n",
       " 0.2016938328742981,\n",
       " 0.23168790340423584,\n",
       " 0.1804683357477188,\n",
       " 0.2569849193096161,\n",
       " 0.5325068235397339,\n",
       " 0.2304120510816574,\n",
       " 0.25868716835975647,\n",
       " 0.1877252757549286,\n",
       " 0.18848355114459991,\n",
       " 0.24687375128269196,\n",
       " 0.5157858729362488,\n",
       " 0.4156588315963745,\n",
       " 0.38757041096687317,\n",
       " 0.3550336956977844,\n",
       " 0.9152860045433044,\n",
       " 0.22453001141548157,\n",
       " 0.1907348334789276,\n",
       " 0.1977468878030777,\n",
       " 0.21196101605892181,\n",
       " 0.21973349153995514,\n",
       " 0.24197714030742645,\n",
       " 0.1983109563589096,\n",
       " 0.22715623676776886,\n",
       " 0.21982812881469727,\n",
       " 0.911647379398346,\n",
       " 0.23644694685935974,\n",
       " 0.20899654924869537,\n",
       " 0.1980578452348709,\n",
       " 0.24092359840869904,\n",
       " 0.18617597222328186,\n",
       " 0.47044265270233154,\n",
       " 0.40499141812324524,\n",
       " 0.6226182579994202,\n",
       " 0.4265904724597931,\n",
       " 0.8839737176895142,\n",
       " 0.9021033644676208,\n",
       " 0.2230183333158493,\n",
       " 0.8175963163375854,\n",
       " 0.459168404340744,\n",
       " 0.4268726706504822,\n",
       " 0.32148250937461853,\n",
       " 0.3129766583442688,\n",
       " 0.5255038738250732,\n",
       " 0.6618818044662476,\n",
       " 0.47527292370796204,\n",
       " 0.9242734909057617,\n",
       " 0.1755271852016449,\n",
       " 0.18593455851078033,\n",
       " 0.17185713350772858,\n",
       " 0.6816667914390564,\n",
       " 0.7213380932807922,\n",
       " 0.846648633480072,\n",
       " 0.858245313167572,\n",
       " 0.8355875611305237,\n",
       " 0.8433342576026917,\n",
       " 0.21369823813438416,\n",
       " 0.1773691177368164,\n",
       " 0.6397291421890259,\n",
       " 0.7628338932991028,\n",
       " 0.7690033316612244,\n",
       " 0.8299174308776855,\n",
       " 0.847592294216156,\n",
       " 0.871222734451294,\n",
       " 0.9013753533363342,\n",
       " 0.20575779676437378,\n",
       " 0.165792316198349,\n",
       " 0.23676592111587524,\n",
       " 0.5762904286384583,\n",
       " 0.7953953146934509,\n",
       " 0.8086484670639038,\n",
       " 0.7908797264099121,\n",
       " 0.8764498829841614,\n",
       " 0.8384472727775574,\n",
       " 0.9023374319076538,\n",
       " 0.8945065140724182,\n",
       " 0.5466211438179016,\n",
       " 0.8425782322883606,\n",
       " 0.8470234870910645,\n",
       " 0.5585187077522278,\n",
       " 0.4321330487728119,\n",
       " 0.3791705369949341,\n",
       " 0.6931856274604797,\n",
       " 0.7657224535942078,\n",
       " 0.9134169220924377,\n",
       " 0.26212528347969055,\n",
       " 0.22738368809223175,\n",
       " 0.7104383707046509,\n",
       " 0.4904332756996155,\n",
       " 0.37417516112327576,\n",
       " 0.3420093357563019,\n",
       " 0.6054860949516296,\n",
       " 0.45923906564712524,\n",
       " 0.6285901069641113,\n",
       " 0.24795736372470856,\n",
       " 0.23281243443489075,\n",
       " 0.16701175272464752,\n",
       " 0.20747607946395874,\n",
       " 0.2081047147512436,\n",
       " 0.1639086902141571,\n",
       " 0.18162602186203003,\n",
       " 0.47997668385505676,\n",
       " 0.5117673873901367,\n",
       " 0.18887244164943695,\n",
       " 0.22344809770584106,\n",
       " 0.2191689908504486,\n",
       " 0.18352574110031128,\n",
       " 0.23011843860149384,\n",
       " 0.20377402007579803,\n",
       " 0.21054711937904358,\n",
       " 0.19835206866264343,\n",
       " 0.44217994809150696,\n",
       " 0.9095224142074585,\n",
       " 0.8670014142990112,\n",
       " 0.9207566976547241,\n",
       " 0.878343403339386,\n",
       " 0.9307608604431152,\n",
       " 0.9061139225959778,\n",
       " 0.9062449336051941,\n",
       " 0.17923402786254883,\n",
       " 0.1704976111650467,\n",
       " 0.2153150737285614,\n",
       " 0.5720798373222351,\n",
       " 0.8122554421424866,\n",
       " 0.7942379713058472,\n",
       " 0.5287662744522095,\n",
       " 0.6995773315429688,\n",
       " 0.8502854108810425,\n",
       " 0.20243604481220245,\n",
       " 0.17354029417037964,\n",
       " 0.19112785160541534,\n",
       " 0.6328359246253967,\n",
       " 0.6805420517921448,\n",
       " 0.8475135564804077,\n",
       " 0.8452613949775696,\n",
       " 0.6461515426635742,\n",
       " 0.5456029176712036,\n",
       " 0.900404155254364,\n",
       " 0.20663516223430634,\n",
       " 0.18703463673591614,\n",
       " 0.1685989946126938,\n",
       " 0.17716684937477112,\n",
       " 0.20189934968948364,\n",
       " 0.5408244729042053,\n",
       " 0.6298658847808838,\n",
       " 0.5227596759796143,\n",
       " 0.582726001739502,\n",
       " 0.9181581735610962,\n",
       " 0.8779585957527161,\n",
       " 0.8979541659355164,\n",
       " 0.9024601578712463,\n",
       " 0.1695249080657959,\n",
       " 0.1731959879398346,\n",
       " 0.6944198608398438,\n",
       " 0.5041518807411194,\n",
       " 0.4165565073490143,\n",
       " 0.3912058472633362,\n",
       " 0.5412718057632446,\n",
       " 0.44633349776268005,\n",
       " 0.5700535178184509,\n",
       " 0.1892911046743393,\n",
       " 0.1766110360622406,\n",
       " 0.20356334745883942,\n",
       " 0.20192573964595795,\n",
       " 0.2058601826429367,\n",
       " 0.20059211552143097,\n",
       " 0.22213377058506012,\n",
       " 0.1960708647966385,\n",
       " 0.21857640147209167,\n",
       " 0.9078872203826904,\n",
       " 0.2134779691696167,\n",
       " 0.18859295547008514,\n",
       " 0.18382693827152252,\n",
       " 0.5377627611160278,\n",
       " 0.37771910429000854,\n",
       " 0.6957735419273376,\n",
       " 0.48853880167007446,\n",
       " 0.3543049097061157,\n",
       " 0.3154835104942322,\n",
       " 0.9246342182159424,\n",
       " 0.901637077331543,\n",
       " 0.8993237018585205,\n",
       " 0.9209966659545898,\n",
       " 0.8765535950660706,\n",
       " 0.8925861120223999,\n",
       " 0.9169823527336121,\n",
       " 0.944517970085144,\n",
       " 0.2173275500535965,\n",
       " 0.17574350535869598,\n",
       " 0.2489899843931198,\n",
       " 0.24480649828910828,\n",
       " 0.1961558610200882,\n",
       " 0.20216241478919983,\n",
       " 0.48313015699386597,\n",
       " 0.42359015345573425,\n",
       " 0.5851168036460876,\n",
       " 0.20310291647911072,\n",
       " 0.18094030022621155,\n",
       " 0.21213398873806,\n",
       " 0.19078269600868225,\n",
       " 0.22902418673038483,\n",
       " 0.21593210101127625,\n",
       " 0.4370957911014557,\n",
       " 0.6268376111984253,\n",
       " 0.5383804440498352,\n",
       " 0.8676668405532837,\n",
       " 0.8742141127586365,\n",
       " 0.9111037850379944,\n",
       " 0.8857578039169312,\n",
       " 0.9297453761100769,\n",
       " 0.8825468420982361,\n",
       " 0.9189549684524536,\n",
       " 0.9183415174484253,\n",
       " 0.21210430562496185,\n",
       " 0.8150268197059631,\n",
       " 0.8163602948188782,\n",
       " 0.8039712905883789,\n",
       " 0.8577710390090942,\n",
       " 0.9251811504364014,\n",
       " 0.892792820930481,\n",
       " 0.6121536493301392,\n",
       " 0.7941241264343262,\n",
       " 0.8260939121246338,\n",
       " 0.8702194094657898,\n",
       " 0.8998867273330688,\n",
       " 0.8781155347824097,\n",
       " 0.9144828915596008,\n",
       " 0.1767379194498062,\n",
       " 0.17782747745513916,\n",
       " 0.1992390751838684,\n",
       " 0.20024627447128296,\n",
       " 0.1596665382385254,\n",
       " 0.2035793662071228,\n",
       " 0.18806928396224976,\n",
       " 0.1941230595111847,\n",
       " 0.20217370986938477,\n",
       " 0.929598867893219,\n",
       " 0.9192864298820496,\n",
       " 0.2204916924238205,\n",
       " 0.18753252923488617,\n",
       " 0.16318489611148834,\n",
       " 0.20094417035579681,\n",
       " 0.2413981556892395,\n",
       " 0.19891610741615295,\n",
       " 0.18240684270858765,\n",
       " 0.26195448637008667,\n",
       " 0.22248463332653046,\n",
       " 0.9021398425102234,\n",
       " 0.9281661510467529,\n",
       " 0.8991081714630127,\n",
       " 0.8960943222045898,\n",
       " 0.9123312830924988,\n",
       " 0.8955186605453491,\n",
       " 0.8740744590759277,\n",
       " 0.891137421131134,\n",
       " 0.8790078163146973,\n",
       " 0.9137970805168152,\n",
       " 0.22677572071552277,\n",
       " 0.19321377575397491,\n",
       " 0.18945929408073425,\n",
       " 0.22276926040649414,\n",
       " 0.585432767868042,\n",
       " 0.39907214045524597,\n",
       " 0.39310768246650696,\n",
       " 0.3215007483959198,\n",
       " 0.2878156900405884,\n",
       " 0.879888117313385,\n",
       " 0.8822153210639954,\n",
       " 0.8906867504119873,\n",
       " 0.9051596522331238,\n",
       " 0.8863117098808289,\n",
       " 0.8859403729438782,\n",
       " 0.926601231098175,\n",
       " 0.9185208082199097,\n",
       " 0.900763213634491,\n",
       " 0.9013214707374573,\n",
       " 0.1705394983291626,\n",
       " 0.20801901817321777,\n",
       " 0.2132919728755951,\n",
       " 0.235406294465065,\n",
       " 0.17576858401298523,\n",
       " 0.1951153725385666,\n",
       " 0.17922943830490112,\n",
       " 0.19971181452274323,\n",
       " 0.19044359028339386,\n",
       " 0.21165817975997925,\n",
       " 0.2521711587905884,\n",
       " 0.18224839866161346,\n",
       " 0.5273326635360718,\n",
       " 0.4123421013355255,\n",
       " 0.7248982191085815,\n",
       " 0.8058775067329407,\n",
       " 0.8517081141471863,\n",
       " 0.8133749961853027,\n",
       " 0.9109496474266052,\n",
       " 0.9222113490104675,\n",
       " 0.22263279557228088,\n",
       " 0.23977454006671906,\n",
       " 0.6450557708740234,\n",
       " 0.4354129135608673,\n",
       " 0.3581616282463074,\n",
       " 0.5874599814414978,\n",
       " 0.7743768692016602,\n",
       " 0.8430226445198059,\n",
       " 0.8545839190483093,\n",
       " 0.21658049523830414,\n",
       " 0.6589422821998596,\n",
       " 0.8116928935050964,\n",
       " 0.873288094997406,\n",
       " 0.8207741379737854,\n",
       " 0.6906764507293701,\n",
       " 0.5855432152748108,\n",
       " 0.4912930727005005,\n",
       " 0.6597951054573059,\n",
       " 0.2033870816230774,\n",
       " 0.17670240998268127,\n",
       " 0.20258204638957977,\n",
       " 0.60014408826828,\n",
       " 0.4788270890712738,\n",
       " 0.4140300452709198,\n",
       " 0.6879539489746094,\n",
       " 0.47093695402145386,\n",
       " 0.37661078572273254,\n",
       " 0.9002236723899841,\n",
       " 0.9297268986701965,\n",
       " 0.9000216126441956,\n",
       " 0.8928948640823364,\n",
       " 0.9216219782829285,\n",
       " 0.9155029654502869,\n",
       " 0.2507076859474182,\n",
       " 0.2362195998430252,\n",
       " 0.18096616864204407,\n",
       " 0.2314552366733551,\n",
       " 0.18465054035186768,\n",
       " 0.2319401502609253,\n",
       " 0.2037729024887085,\n",
       " 0.5108944773674011,\n",
       " 0.42768219113349915,\n",
       " 0.9239867925643921,\n",
       " 0.8858469128608704,\n",
       " 0.860674262046814,\n",
       " 0.9099439978599548,\n",
       " 0.18203988671302795,\n",
       " 0.20054492354393005,\n",
       " 0.1813672035932541,\n",
       " 0.25213202834129333,\n",
       " 0.18407012522220612,\n",
       " 0.20352354645729065,\n",
       " 0.17670685052871704,\n",
       " 0.20756807923316956,\n",
       " 0.22038936614990234,\n",
       " 0.9175740480422974,\n",
       " 0.921461820602417,\n",
       " 0.8969519138336182,\n",
       " 0.5241265892982483,\n",
       " 0.8270270228385925,\n",
       " 0.6303876638412476,\n",
       " 0.5210619568824768,\n",
       " 0.7121677994728088,\n",
       " 0.7547150254249573,\n",
       " 0.7905584573745728,\n",
       " 0.5811156034469604,\n",
       " 0.9102669358253479,\n",
       " 0.21266046166419983,\n",
       " 0.8477981090545654,\n",
       " 0.5540006160736084,\n",
       " 0.4680987298488617,\n",
       " 0.366779625415802,\n",
       " 0.22036509215831757,\n",
       " 0.27152949571609497,\n",
       " 0.4272213876247406,\n",
       " 0.4057501554489136,\n",
       " 0.9316571354866028,\n",
       " 0.9010742902755737,\n",
       " 0.21099068224430084,\n",
       " 0.7108235955238342,\n",
       " 0.8697830438613892,\n",
       " 0.8886865973472595,\n",
       " 0.5826281905174255,\n",
       " 0.528512716293335,\n",
       " 0.4421860873699188,\n",
       " 0.36156514286994934,\n",
       " 0.6239124536514282,\n",
       " 0.924344539642334,\n",
       " 0.8910637497901917,\n",
       " 0.5039262771606445,\n",
       " 0.7859975695610046,\n",
       " 0.8603608012199402,\n",
       " 0.9044666290283203,\n",
       " 0.9280466437339783,\n",
       " 0.22409360110759735,\n",
       " 0.16946114599704742,\n",
       " 0.21407848596572876,\n",
       " 0.23207001388072968,\n",
       " 0.19019336998462677,\n",
       " 0.1733698546886444,\n",
       " 0.21526898443698883,\n",
       " 0.21693123877048492,\n",
       " 0.1893286406993866,\n",
       " 0.1684742420911789,\n",
       " 0.15810538828372955,\n",
       " 0.23356766998767853,\n",
       " 0.5451328754425049,\n",
       " 0.6602499485015869,\n",
       " 0.7526065111160278,\n",
       " 0.5176711082458496,\n",
       " 0.6834881901741028,\n",
       " 0.4974935054779053,\n",
       " 0.9167497754096985,\n",
       " 0.8899978399276733,\n",
       " 0.9169450998306274,\n",
       " 0.2262599915266037,\n",
       " 0.23434677720069885,\n",
       " 0.23303982615470886,\n",
       " 0.6053019762039185,\n",
       " 0.7376658916473389,\n",
       " 0.7288457751274109,\n",
       " 0.8106905221939087,\n",
       " 0.8855984210968018,\n",
       " 0.8585209250450134,\n",
       " 0.8659290671348572,\n",
       " 0.9117959141731262,\n",
       " 0.2612253725528717,\n",
       " 0.7603932619094849,\n",
       " 0.8701195120811462,\n",
       " 0.9014790654182434,\n",
       " 0.9157386422157288,\n",
       " 0.8732807040214539,\n",
       " 0.9258411526679993,\n",
       " 0.9241892695426941,\n",
       " 0.9221510291099548,\n",
       " 0.9106895327568054,\n",
       " 0.9169297814369202,\n",
       " 0.9184834957122803,\n",
       " 0.23453742265701294,\n",
       " 0.7403558492660522,\n",
       " 0.8640660643577576,\n",
       " 0.8602092862129211,\n",
       " 0.5940372347831726,\n",
       " 0.7976832985877991,\n",
       " 0.7694446444511414,\n",
       " 0.8880422711372375,\n",
       " 0.9131661057472229,\n",
       " 0.918644905090332,\n",
       " 0.9101255536079407,\n",
       " 0.9118496179580688,\n",
       " 0.912631630897522,\n",
       " 0.9195438027381897,\n",
       " 0.9211975336074829,\n",
       " 0.9315746426582336,\n",
       " 0.1898803859949112,\n",
       " 0.19625961780548096,\n",
       " 0.1797633320093155,\n",
       " 0.6649835705757141,\n",
       " 0.738967776298523,\n",
       " 0.7417991161346436,\n",
       " 0.8607469201087952,\n",
       " 0.8582258820533752,\n",
       " 0.6706740856170654,\n",
       " 0.9368510246276855,\n",
       " 0.19318155944347382,\n",
       " 0.7837246060371399,\n",
       " 0.4971281588077545,\n",
       " 0.41067689657211304,\n",
       " 0.3290804624557495,\n",
       " 0.30223631858825684,\n",
       " 0.2277834117412567,\n",
       " 0.23269422352313995,\n",
       " 0.22763346135616302,\n",
       " 0.20147804915905,\n",
       " 0.17575152218341827,\n",
       " 0.16746638715267181,\n",
       " 0.24735127389431,\n",
       " 0.17943687736988068,\n",
       " 0.21399231255054474,\n",
       " 0.23732921481132507,\n",
       " 0.2182583063840866,\n",
       " 0.2354060560464859,\n",
       " 0.9042247533798218,\n",
       " 0.19283069670200348,\n",
       " 0.7336280345916748,\n",
       " 0.5761366486549377,\n",
       " 0.7454013228416443,\n",
       " 0.7684274911880493,\n",
       " 0.8212786912918091,\n",
       " 0.8465402126312256,\n",
       " 0.5570493340492249,\n",
       " 0.7895039319992065,\n",
       " 0.8982122540473938,\n",
       " 0.9004786610603333,\n",
       " 0.9004201889038086,\n",
       " 0.1983523666858673,\n",
       " 0.20114509761333466,\n",
       " 0.18370109796524048,\n",
       " 0.17276476323604584,\n",
       " 0.5537068843841553,\n",
       " 0.47685450315475464,\n",
       " 0.6464548110961914,\n",
       " 0.6882054805755615,\n",
       " 0.7918875217437744,\n",
       " 0.8828160166740417,\n",
       " 0.85718834400177,\n",
       " 0.9406875967979431,\n",
       " 0.9336141347885132,\n",
       " 0.9415157437324524,\n",
       " 0.9191724061965942,\n",
       " 0.9026603102684021,\n",
       " 0.8853275775909424,\n",
       " 0.8636454343795776,\n",
       " 0.9260196089744568,\n",
       " 0.21701611578464508,\n",
       " 0.7113243341445923,\n",
       " 0.7434300780296326,\n",
       " 0.4777590036392212,\n",
       " 0.7339617609977722,\n",
       " 0.8697744011878967,\n",
       " 0.793030321598053,\n",
       " 0.9093531966209412,\n",
       " 0.9163398146629333,\n",
       " 0.8849400877952576,\n",
       " 0.8989348411560059,\n",
       " 0.9455068707466125,\n",
       " 0.22662121057510376,\n",
       " 0.2253471314907074,\n",
       " 0.2486717700958252,\n",
       " 0.1809464693069458,\n",
       " 0.18731461465358734,\n",
       " 0.2316364347934723,\n",
       " 0.1863633543252945,\n",
       " 0.23865178227424622,\n",
       " 0.22238358855247498,\n",
       " 0.9387882351875305,\n",
       " 0.8946829438209534,\n",
       " 0.9045678973197937,\n",
       " 0.8756052851676941,\n",
       " 0.9062870144844055,\n",
       " 0.935983419418335,\n",
       " 0.1908944547176361,\n",
       " 0.25307995080947876,\n",
       " 0.17550376057624817,\n",
       " 0.1939868927001953,\n",
       " 0.5415869355201721,\n",
       " 0.4000094532966614,\n",
       " 0.40170639753341675,\n",
       " 0.33456432819366455,\n",
       " 0.49185818433761597,\n",
       " 0.9221822619438171,\n",
       " 0.9079055786132812,\n",
       " 0.20136867463588715,\n",
       " 0.713292121887207,\n",
       " 0.8348618149757385,\n",
       " 0.8619425892829895,\n",
       " 0.8540467619895935,\n",
       " 0.5318824648857117,\n",
       " 0.46837639808654785,\n",
       " 0.6935920119285583,\n",
       " 0.7607178688049316,\n",
       " 0.9312433004379272,\n",
       " 0.18892820179462433,\n",
       " 0.19934113323688507,\n",
       " 0.1916203796863556,\n",
       " 0.23964937031269073,\n",
       " 0.2538284957408905,\n",
       " 0.22159260511398315,\n",
       " 0.20863111317157745,\n",
       " 0.2069561630487442,\n",
       " 0.23440021276474,\n",
       " 0.2143394500017166,\n",
       " 0.19151100516319275,\n",
       " 0.19927054643630981,\n",
       " 0.21420025825500488,\n",
       " 0.16912762820720673,\n",
       " 0.20400060713291168,\n",
       " 0.556671679019928,\n",
       " 0.40065470337867737,\n",
       " 0.4240223169326782,\n",
       " 0.9196380376815796,\n",
       " 0.19269223511219025,\n",
       " 0.20306353271007538,\n",
       " 0.1760372370481491,\n",
       " 0.19425232708454132,\n",
       " 0.6067453622817993,\n",
       " 0.39651334285736084,\n",
       " 0.65140300989151,\n",
       " 0.550852358341217,\n",
       " 0.43423333764076233,\n",
       " 0.9171397686004639,\n",
       " 0.899570882320404,\n",
       " 0.9341616630554199,\n",
       " 0.9397070407867432,\n",
       " 0.890975832939148,\n",
       " 0.925018310546875,\n",
       " 0.9344077110290527,\n",
       " 0.925886869430542,\n",
       " 0.9229850769042969,\n",
       " 0.19739337265491486,\n",
       " 0.23914088308811188,\n",
       " 0.21843649446964264,\n",
       " 0.22083555161952972,\n",
       " 0.21959412097930908,\n",
       " 0.15881483256816864,\n",
       " 0.1968984305858612,\n",
       " 0.44575878977775574,\n",
       " 0.6865652799606323,\n",
       " 0.9173434972763062,\n",
       " 0.17301347851753235,\n",
       " 0.20605002343654633,\n",
       " 0.19264550507068634,\n",
       " 0.5457102060317993,\n",
       " 0.7184572815895081,\n",
       " 0.8370637893676758,\n",
       " 0.8612561225891113,\n",
       " 0.8750203847885132,\n",
       " 0.6176074743270874,\n",
       " 0.9266157746315002,\n",
       " 0.907814085483551,\n",
       " 0.9142269492149353,\n",
       " 0.8818184733390808,\n",
       " 0.5276128649711609,\n",
       " 0.7637990117073059,\n",
       " 0.8649305701255798,\n",
       " 0.8996864557266235,\n",
       " 0.8748166561126709,\n",
       " 0.7108089327812195,\n",
       " 0.6033380031585693,\n",
       " 0.4694671332836151,\n",
       " 0.9277908802032471,\n",
       " 0.9063136577606201,\n",
       " 0.9054729342460632,\n",
       " 0.9217268824577332,\n",
       " 0.9266029596328735,\n",
       " 0.8957403302192688,\n",
       " 0.9139235615730286,\n",
       " 0.9091192483901978,\n",
       " 0.936470091342926,\n",
       " 0.8860167264938354,\n",
       " 0.8758171200752258,\n",
       " 0.8887761831283569,\n",
       " 0.6280366778373718,\n",
       " 0.48362070322036743,\n",
       " 0.49047720432281494,\n",
       " 0.5862513780593872,\n",
       " 0.8007650971412659,\n",
       " 0.817035436630249,\n",
       " 0.1915501356124878,\n",
       " 0.17534734308719635,\n",
       " 0.2178245484828949,\n",
       " 0.2209915667772293,\n",
       " 0.19934235513210297,\n",
       " 0.5610116124153137,\n",
       " 0.668979823589325,\n",
       " 0.6419715881347656,\n",
       " 0.8180477023124695,\n",
       " 0.8962650299072266,\n",
       " 0.8909218311309814,\n",
       " 0.9111995697021484,\n",
       " 0.9116095900535583,\n",
       " 0.16575057804584503,\n",
       " 0.8063669800758362,\n",
       " 0.4231559634208679,\n",
       " 0.39212340116500854,\n",
       " 0.32416030764579773,\n",
       " 0.605935275554657,\n",
       " 0.5525610446929932,\n",
       " 0.6185749173164368,\n",
       " 0.6817329525947571,\n",
       " 0.1871120184659958,\n",
       " 0.7676030993461609,\n",
       " 0.4987197518348694,\n",
       " 0.7454991340637207,\n",
       " 0.8038936257362366,\n",
       " 0.8191803097724915,\n",
       " 0.8134520649909973,\n",
       " 0.8689702749252319,\n",
       " 0.899186909198761,\n",
       " 0.2089778482913971,\n",
       " 0.20201973617076874,\n",
       " 0.7548404335975647,\n",
       " 0.7674893736839294,\n",
       " 0.4811733067035675,\n",
       " 0.750170111656189,\n",
       " 0.7870936989784241,\n",
       " 0.8264702558517456,\n",
       " 0.8942215442657471,\n",
       " 0.8758907318115234,\n",
       " 0.8932831883430481,\n",
       " 0.8926381468772888,\n",
       " 0.8902918100357056,\n",
       " 0.8951952457427979,\n",
       " 0.9431652426719666,\n",
       " 0.9090335369110107,\n",
       " 0.19327400624752045,\n",
       " 0.18277280032634735,\n",
       " 0.7752161622047424,\n",
       " 0.7868207693099976,\n",
       " 0.5539615750312805,\n",
       " 0.4727697968482971,\n",
       " 0.7071176767349243,\n",
       " 0.728178083896637,\n",
       " 0.8269716501235962,\n",
       " 0.868252158164978,\n",
       " 0.8717947006225586,\n",
       " 0.9206216931343079,\n",
       " 0.8915090560913086,\n",
       " 0.5143361687660217,\n",
       " 0.4130731225013733,\n",
       " 0.6810425519943237,\n",
       " 0.559256911277771,\n",
       " 0.7626094222068787,\n",
       " 0.8390740156173706,\n",
       " 0.8253916501998901,\n",
       " 0.5362420678138733,\n",
       " 0.2307332456111908,\n",
       " 0.19574861228466034,\n",
       " 0.18696096539497375,\n",
       " 0.22529558837413788,\n",
       " 0.16685350239276886,\n",
       " 0.2098785638809204,\n",
       " 0.16966162621974945,\n",
       " 0.2055598497390747,\n",
       " 0.22015036642551422,\n",
       " 0.9071499705314636,\n",
       " 0.16650809347629547,\n",
       " 0.19806483387947083,\n",
       " 0.17126542329788208,\n",
       " 0.22118745744228363,\n",
       " 0.18654927611351013,\n",
       " 0.21883006393909454,\n",
       " 0.5096408128738403,\n",
       " 0.4116993546485901,\n",
       " 0.35765960812568665,\n",
       " 0.2138589322566986,\n",
       " 0.18769966065883636,\n",
       " 0.20789575576782227,\n",
       " 0.6686858534812927,\n",
       " 0.5340741276741028,\n",
       " 0.3810388445854187,\n",
       " 0.31799373030662537,\n",
       " 0.5969691276550293,\n",
       " 0.44843393564224243,\n",
       " 0.20702774822711945,\n",
       " 0.21200309693813324,\n",
       " 0.20540598034858704,\n",
       " 0.20915596187114716,\n",
       " 0.5587521195411682,\n",
       " 0.6892622709274292,\n",
       " 0.663644015789032,\n",
       " 0.8226341009140015,\n",
       " 0.569311797618866,\n",
       " 0.8877003192901611,\n",
       " 0.5200589895248413,\n",
       " 0.7947173714637756,\n",
       " 0.8566505908966064,\n",
       " 0.8617745041847229,\n",
       " 0.8497937321662903,\n",
       " 0.8731089234352112,\n",
       " 0.9007708430290222,\n",
       " 0.8884950876235962,\n",
       " 0.914583683013916,\n",
       " 0.9201866984367371,\n",
       " 0.900924026966095,\n",
       " 0.9326533675193787,\n",
       " 0.9405494332313538,\n",
       " 0.20167405903339386,\n",
       " 0.6677257418632507,\n",
       " 0.7987757325172424,\n",
       " 0.877564549446106,\n",
       " 0.8767911195755005,\n",
       " 0.8892776966094971,\n",
       " 0.8635054230690002,\n",
       " 0.8753523826599121,\n",
       " 0.8935359716415405,\n",
       " 0.908008873462677,\n",
       " 0.9139308929443359,\n",
       " 0.9126462340354919,\n",
       " 0.8454410433769226,\n",
       " 0.5540910959243774,\n",
       " 0.36416223645210266,\n",
       " 0.37432265281677246,\n",
       " 0.32755187153816223,\n",
       " 0.2413032203912735,\n",
       " 0.23009508848190308,\n",
       " 0.2803710401058197,\n",
       " 0.28809332847595215,\n",
       " 0.19353030622005463,\n",
       " 0.21400071680545807,\n",
       " 0.18538376688957214,\n",
       " 0.24682702124118805,\n",
       " 0.18626677989959717,\n",
       " 0.22805389761924744,\n",
       " 0.1964561641216278,\n",
       " 0.1806783527135849,\n",
       " 0.21034206449985504,\n",
       " 0.9320029616355896,\n",
       " 0.8685869574546814,\n",
       " 0.9019569158554077,\n",
       " 0.8898351788520813,\n",
       " 0.9072631597518921,\n",
       " 0.17812035977840424,\n",
       " 0.22412680089473724,\n",
       " 0.2003335803747177,\n",
       " 0.19909873604774475,\n",
       " 0.19996072351932526,\n",
       " 0.19652067124843597,\n",
       " 0.20012164115905762,\n",
       " 0.20236364006996155,\n",
       " 0.17855487763881683,\n",
       " 0.8753119111061096,\n",
       " 0.9030202031135559,\n",
       " 0.8849357962608337,\n",
       " 0.8806430697441101,\n",
       " 0.9030414819717407,\n",
       " 0.9463158249855042,\n",
       " 0.905755877494812,\n",
       " 0.9009373784065247,\n",
       " 0.921815037727356,\n",
       " 0.9468651413917542,\n",
       " 0.9069106578826904,\n",
       " 0.8849437236785889,\n",
       " 0.9134382009506226,\n",
       " 0.9067955613136292,\n",
       " 0.2545819878578186,\n",
       " 0.20651927590370178,\n",
       " 0.23024387657642365,\n",
       " 0.17665810883045197,\n",
       " 0.21387863159179688,\n",
       " 0.187827929854393,\n",
       " 0.23131823539733887,\n",
       " 0.589741587638855,\n",
       " 0.4593106806278229,\n",
       " 0.9214733839035034,\n",
       " 0.23769629001617432,\n",
       " 0.19101043045520782,\n",
       " 0.2515368163585663,\n",
       " 0.22719869017601013,\n",
       " 0.19070519506931305,\n",
       " 0.509739339351654,\n",
       " 0.6000285744667053,\n",
       " 0.4225439131259918,\n",
       " 0.40204131603240967,\n",
       " 0.879567563533783,\n",
       " 0.8883673548698425,\n",
       " 0.8643019795417786,\n",
       " 0.9276671409606934,\n",
       " 0.8938912749290466,\n",
       " 0.9097403883934021,\n",
       " 0.8926007151603699,\n",
       " 0.9150430560112,\n",
       " 0.9174903631210327,\n",
       " 0.16572602093219757,\n",
       " 0.17087393999099731,\n",
       " 0.22985196113586426,\n",
       " 0.6063471436500549,\n",
       " 0.43614616990089417,\n",
       " 0.38911187648773193,\n",
       " 0.3469264507293701,\n",
       " 0.24917660653591156,\n",
       " 0.4446212947368622,\n",
       " 0.21500708162784576,\n",
       " 0.20575129985809326,\n",
       " 0.22602955996990204,\n",
       " 0.18878288567066193,\n",
       " 0.5436837673187256,\n",
       " 0.4276767671108246,\n",
       " 0.41618651151657104,\n",
       " 0.665926992893219,\n",
       " 0.6406514644622803,\n",
       " 0.8656708002090454,\n",
       " 0.8958271741867065,\n",
       " 0.6831538081169128,\n",
       " 0.816082775592804,\n",
       " 0.8300028443336487,\n",
       " 0.8805017471313477,\n",
       " 0.6011623740196228,\n",
       " 0.5303251147270203,\n",
       " 0.5601392388343811,\n",
       " 0.9157695770263672,\n",
       " 0.8821648359298706,\n",
       " 0.9172908067703247,\n",
       " 0.8601800203323364,\n",
       " 0.5335584282875061,\n",
       " 0.3776836693286896,\n",
       " 0.6768942475318909,\n",
       " 0.7721271514892578,\n",
       " 0.8245037794113159,\n",
       " 0.9026432037353516,\n",
       " 0.9186464548110962,\n",
       " 0.9150819182395935,\n",
       " 0.8702978491783142,\n",
       " 0.8836334347724915,\n",
       " 0.9138791561126709,\n",
       " 0.896157443523407,\n",
       " 0.905480146408081,\n",
       " 0.8676280379295349,\n",
       " 0.8829306960105896,\n",
       " 0.9079145193099976,\n",
       " 0.9014772176742554,\n",
       " 0.18070416152477264,\n",
       " 0.18862153589725494,\n",
       " 0.24896997213363647,\n",
       " 0.6889733076095581,\n",
       " 0.7865945100784302,\n",
       " 0.7400179505348206,\n",
       " 0.8061982989311218,\n",
       " 0.6141703724861145,\n",
       " 0.7334243059158325]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9127399921417236\n"
     ]
    }
   ],
   "source": [
    "print(total_rewards[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "Counter([i[0] for i in obs_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/akt_pid/assist2009_pid/args.pkl', 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "env = PracticeProblemEnv(params,max_step=1000, rew_func='greedy',device=device)\n",
    "obs, _ = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, terminated, truncated, info = env.step(np.array(75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.q_p_dict\n",
    "sampled_concpets = []\n",
    "sampled_problems = []\n",
    "n_problems_per_type = 10\n",
    "for question_type, question_ids in env.q_p_dict.items():\n",
    "    num = min(n_problems_per_type, len(question_ids))\n",
    "    sampled_problems += random.sample([*question_ids], num)\n",
    "    sampled_concpets += ([question_type ] * num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_problems = sampled_problems[:1]\n",
    "# sampled_concpets = sampled_concpets[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_n_problem = len(sampled_problems)\n",
    "q = torch.cat((torch.tensor(env.history['q'][-(env.params.seqlen-1):]).tile((test_n_problem,1)),(torch.tensor(sampled_concpets).unsqueeze(-1))),1)\n",
    "target = torch.tensor(env.history['target'][-(env.params.seqlen-1):]+[0]).tile((test_n_problem,1))\n",
    "pid = torch.cat((torch.tensor(env.history['pid'][-(env.params.seqlen-1):]).tile((test_n_problem,1)),(torch.tensor(sampled_problems).unsqueeze(-1))),1)\n",
    "assert pid.shape == target.shape == pid.shape #(test_n_problem,3)\n",
    "qa = q+target*env.params.n_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_q = torch.zeros((test_n_problem, env.params.seqlen)) \n",
    "padded_qa = torch.zeros((test_n_problem, env.params.seqlen))\n",
    "padded_target = torch.full((test_n_problem,env.params.seqlen),-1)\n",
    "padded_pid = torch.zeros((test_n_problem, env.params.seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = q.shape[1]\n",
    "padded_q[:, :pred_index]= q\n",
    "padded_qa[:, :pred_index]= qa\n",
    "padded_target[:, :pred_index]= target\n",
    "padded_pid[:, :pred_index]= pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = padded_q.long().to(device)\n",
    "qa = padded_qa.long().to(device)\n",
    "target = padded_target.long().to(device)\n",
    "pid = padded_pid.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss, pred, ct = env.kt_model(q,qa,target,pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopadding_index = np.flatnonzero(padded_target.reshape((-1,)) >= -0.9).tolist()\n",
    "pred_nopadding = pred[nopadding_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pred_nopadding[(pred_index-1)::pred_index]\n",
    "assert test_result.shape == (test_n_problem,)\n",
    "mean_performance = test_result.mean().item()\n",
    "# return mean_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([750])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.418801873922348"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(3).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
