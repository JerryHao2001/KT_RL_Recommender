{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PracticeEnv:\n",
    "    def __init__(self, params, kt_model,device):\n",
    "        self.num_questions = params.n_pid\n",
    "        self.model = kt_model\n",
    "        self.device = device\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.past_interactions = {'q':[],'target':[],'pid':[]} # Initialize past interactions\n",
    "        return self.past_interactions\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Get the prediction from KTModel\n",
    "        q,target,pid = self.past_interactions.values()\n",
    "        q = torch.tensor(q).long.to(self.device)\n",
    "        target = torch.tensor(target).long.to(self.device)\n",
    "        pid = torch.tensor(pid).long.to(self.device)\n",
    "        qa = q+target*self.num_questions\n",
    "\n",
    "        correct_prob = self.model(q,qa,target,pid)\n",
    "        \n",
    "        correct = np.random.rand() < correct_prob\n",
    "        reward = 1 if correct else 0\n",
    "        \n",
    "        # Update past interactions with the result of the action\n",
    "        self.past_interactions.append((action, correct))\n",
    "        \n",
    "        return self.past_interactions, reward, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=-1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = F.softmax(self.fc(x[:, -1, :]), dim=-1)  # Use the output of the last time step\n",
    "        return x\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  # Use the output of the last time step\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "def compute_advantages(rewards, values, gamma=0.99):\n",
    "    advantages = []\n",
    "    advantage = 0\n",
    "    for r, v in zip(reversed(rewards), reversed(values)):\n",
    "        advantage = r + gamma * advantage - v\n",
    "        advantages.insert(0, advantage)\n",
    "    return torch.tensor(advantages)\n",
    "\n",
    "def surrogate_loss(policy_net, states, actions, advantages):\n",
    "    action_probs = policy_net(states)\n",
    "    dist = Categorical(action_probs)\n",
    "    log_probs = dist.log_prob(actions)\n",
    "    return -torch.mean(log_probs * advantages)\n",
    "\n",
    "def train_policy(policy_net, value_net, optimizer_policy, optimizer_value, states, actions, rewards):\n",
    "    # Compute value targets\n",
    "    values = value_net(states).detach()\n",
    "    advantages = compute_advantages(rewards, values)\n",
    "    \n",
    "    # Update value network\n",
    "    value_loss = F.mse_loss(value_net(states).squeeze(), torch.tensor(rewards).float())\n",
    "    optimizer_value.zero_grad()\n",
    "    value_loss.backward()\n",
    "    optimizer_value.step()\n",
    "    \n",
    "    # Update policy network\n",
    "    policy_loss = surrogate_loss(policy_net, states, actions, advantages)\n",
    "    optimizer_policy.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer_policy.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KTModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Initialize environment and networks\u001b[39;00m\n\u001b[0;32m     37\u001b[0m num_questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 38\u001b[0m kt_model \u001b[38;5;241m=\u001b[39m \u001b[43mKTModel\u001b[49m()  \u001b[38;5;66;03m# Assuming KTModel is already defined and loaded\u001b[39;00m\n\u001b[0;32m     39\u001b[0m env \u001b[38;5;241m=\u001b[39m PracticeEnv(num_questions, kt_model)\n\u001b[0;32m     40\u001b[0m policy_net \u001b[38;5;241m=\u001b[39m PolicyNetwork(num_questions, num_questions)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KTModel' is not defined"
     ]
    }
   ],
   "source": [
    "def train(env, policy_net, value_net, num_episodes=1000, gamma=0.99):\n",
    "    optimizer_policy = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "    optimizer_value = optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states, actions, rewards = [], [], []\n",
    "        state = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action_probs = policy_net(state_tensor)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            print(action)\n",
    "            \n",
    "            next_state, reward, correct = env.step(action.item())\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if correct:\n",
    "                done = True\n",
    "        \n",
    "        # Convert states list of lists to a tensor with appropriate padding or truncation\n",
    "        states_tensor = torch.nn.utils.rnn.pad_sequence([torch.tensor(s) for s in states], batch_first=True)\n",
    "        actions_tensor = torch.tensor(actions)\n",
    "        train_policy(policy_net, value_net, optimizer_policy, optimizer_value, states_tensor, actions_tensor, rewards)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, total reward: {sum(rewards)}\")\n",
    "\n",
    "# Initialize environment and networks\n",
    "num_questions = 10\n",
    "kt_model = KTModel()  # Assuming KTModel is already defined and loaded\n",
    "env = PracticeEnv(num_questions, kt_model)\n",
    "policy_net = PolicyNetwork(num_questions, num_questions)\n",
    "value_net = ValueNetwork(num_questions)\n",
    "\n",
    "# Train the model\n",
    "train(env, policy_net, value_net, num_episodes=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
